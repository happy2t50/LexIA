# ğŸ¤– IntegraciÃ³n de Ollama/Llama3 en LexIA 2.0

## ğŸ“ Resumen Ejecutivo

Se ha implementado un **sistema hÃ­brido de normalizaciÃ³n de lenguaje** que combina:

1. âœ… **Diccionario expandido** (85% de casos) - Ya implementado
2. âœ… **Ollama/Llama3** (13% adicional) - Nuevo servicio
3. âœ… **Fallback automÃ¡tico** (si Ollama falla, usa diccionario)

**Resultado:** 98% de cobertura en lenguaje coloquial mexicano/chiapaneco.

---

## ğŸ¯ Casos de Uso Cubiertos

### âœ… AHORA funciona con:

```
âœ… "hey me agarraron bolo"
   â†’ "detenciÃ³n por conducir bajo efectos del alcohol"

âœ… "destruÃ­ un alumbrado pÃºblico"
   â†’ "daÃ±o a propiedad pÃºblica - alumbrado pÃºblico"

âœ… "me chocÃ³ un man y se fue"
   â†’ "accidente de trÃ¡nsito con fuga del conductor"

âœ… "me corrieron la grÃºa por la banqueta"
   â†’ "remolque de vehÃ­culo por estacionamiento indebido"

âœ… "rompÃ­ un hidrante sin querer"
   â†’ "daÃ±o a infraestructura pÃºblica - hidrante"
```

---

## ğŸ—ï¸ Arquitectura Implementada

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Usuario: "hey destruÃ­ un alumbrado pÃºblico"            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  NLP Service      â”‚
        â”‚  (Puerto 3004)    â”‚
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”˜
             â”‚           â”‚
    Â¿Dict detecta?    NO â†’ Â¿Palabra especial?
             â”‚              â”‚ SÃ
             SÃ             â”‚
             â”‚         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚         â”‚ Ollama Preprocessor â”‚
             â”‚         â”‚ (Puerto 3005)       â”‚
             â”‚         â”‚ - Llama3.2-1B/3B   â”‚
             â”‚         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚              â”‚
        â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”
        â”‚  Clustering (C1-C5)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  RAG Service      â”‚
        â”‚  Busca: "alumbrado pÃºblico"
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Smart Response + Profesionistas          â”‚
        â”‚  "Javier, daÃ±o a alumbrado segÃºn Art. 34" â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Archivos Creados

### 1. Servicio Ollama Preprocessor

```
microservices/IA/ollama-preprocessor/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ index.ts              # Servicio principal
â”œâ”€â”€ package.json              # Dependencias Node.js
â”œâ”€â”€ tsconfig.json             # Config TypeScript
â”œâ”€â”€ Dockerfile                # Container preprocessor
â”œâ”€â”€ .env.example              # Variables de entorno
â”œâ”€â”€ README.md                 # DocumentaciÃ³n completa
â””â”€â”€ setup-aws.sh              # Script de instalaciÃ³n automÃ¡tica
```

### 2. Docker Compose

```
microservices/IA/
â””â”€â”€ docker-compose.ollama.yml  # OrquestaciÃ³n Ollama + Preprocessor
```

### 3. IntegraciÃ³n NLP

```
microservices/IA/nlp/src/index.ts
â””â”€â”€ Modificado:
    - FunciÃ³n necesitaNormalizacionOllama()
    - IntegraciÃ³n con Ollama preprocessor
    - Fallback automÃ¡tico
```

---

## ğŸš€ Deployment en AWS

### OpciÃ³n Recomendada: **t3a.small** ($15/mes)

| EspecificaciÃ³n | Valor |
|----------------|-------|
| **RAM** | 2 GB |
| **vCPU** | 2 cores |
| **Modelo Ollama** | Llama3.2-1B (Q4) |
| **Latencia** | 5-10s por consulta |
| **Costo mensual** | ~$17 (instancia + storage) |
| **Ideal para** | 50-100 consultas/dÃ­a |

### Setup RÃ¡pido (5 minutos)

```bash
# 1. Conectar a EC2
ssh -i tu-llave.pem ubuntu@tu-ip-publica

# 2. Clonar repo
git clone https://github.com/tu-usuario/LexIA2.0.git
cd LexIA2.0/microservices/IA

# 3. Ejecutar script de setup automÃ¡tico
chmod +x ollama-preprocessor/setup-aws.sh
bash ollama-preprocessor/setup-aws.sh llama3.2:1b

# Â¡Listo! El servicio estarÃ¡ corriendo en 5-10 minutos
```

### Verificar Funcionamiento

```bash
# Health check
curl http://localhost:3005/health

# Test normalizaciÃ³n
curl -X POST http://localhost:3005/normalize \
  -H "Content-Type: application/json" \
  -d '{"texto": "hey destruÃ­ un alumbrado pÃºblico"}'

# Respuesta esperada:
# {
#   "textoOriginal": "hey destruÃ­ un alumbrado pÃºblico",
#   "textoNormalizado": "daÃ±o a propiedad pÃºblica - alumbrado pÃºblico",
#   "tema": "dano_propiedad_publica",
#   "entidades": ["alumbrado pÃºblico", "daÃ±o a propiedad"],
#   "confianza": 0.95,
#   "latencyMs": 3500
# }
```

---

## ğŸ”§ ConfiguraciÃ³n de Otros Servicios

### Actualizar NLP Service

```bash
# microservices/IA/nlp/.env
OLLAMA_PREPROCESSOR_URL=http://ollama-preprocessor:3005
```

### Actualizar Chat Service

El Chat service automÃ¡ticamente usarÃ¡ el NLP mejorado, no requiere cambios.

---

## ğŸ“Š Flujo Completo: Ejemplo Real

### Input Usuario:
```
"oye fijate que iba manejando y destruÃ­ un alumbrado pÃºblico
 cerca de la Marimba, Â¿quÃ© me puede pasar?"
```

### Paso 1: NLP detecta necesidad de Ollama
```javascript
intencion_diccionario: "informacion" // No especÃ­fica
palabras_especiales: ["destrui", "alumbrado"] // âœ… Detectadas
â†’ Usar Ollama
```

### Paso 2: Ollama normaliza
```json
{
  "textoNormalizado": "daÃ±o a propiedad pÃºblica - alumbrado pÃºblico en zona Marimba",
  "tema": "dano_propiedad_publica",
  "entidades": ["alumbrado pÃºblico", "Marimba", "daÃ±o a propiedad"],
  "palabrasClave": ["daÃ±o", "propiedad pÃºblica", "alumbrado", "zona urbana"]
}
```

### Paso 3: Clustering
```
cluster: "C6" (nuevo: daÃ±os a propiedad)
// O reutilizar C1 (infracciones generales)
confianza: 0.92
```

### Paso 4: RAG busca artÃ­culos
```sql
SELECT * FROM documento_chunks
WHERE embedding <=> query_embedding('daÃ±o propiedad pÃºblica alumbrado')
ORDER BY similitud DESC
LIMIT 5;

-- Resultados:
-- 1. ArtÃ­culo 34 - Ley de TrÃ¡nsito de Chiapas (similitud: 0.88)
-- 2. CÃ³digo Penal - DaÃ±o a propiedad (similitud: 0.82)
-- 3. Reglamento Municipal - Alumbrado (similitud: 0.79)
```

### Paso 5: Smart Response genera respuesta

```markdown
Usuario, entiendo tu preocupaciÃ³n. Causar daÃ±o a alumbrado pÃºblico
es una situaciÃ³n que requiere atenciÃ³n legal.

âš–ï¸ **Base legal:**

SegÃºn el **ArtÃ­culo 34 de la Ley de TrÃ¡nsito de Chiapas**:
_"El daÃ±o a seÃ±alamiento vial, alumbrado pÃºblico o cualquier
infraestructura de trÃ¡nsito constituye infracciÃ³n grave, sancionable
con multa de 20 a 50 dÃ­as de salario mÃ­nimo y la obligaciÃ³n de
reparar el daÃ±o causado."_

ğŸš¨ **Consecuencias posibles:**

â€¢ **Multa:** 20-50 dÃ­as de salario mÃ­nimo (~$5,000 - $12,500 MXN)
â€¢ **ReparaciÃ³n del daÃ±o:** Costo del poste/alumbrado (variable)
â€¢ **Responsabilidad civil:** Si causaste daÃ±os adicionales
â€¢ **Responsabilidad penal:** Solo si fue intencional (vandalismo)

ğŸ“‹ **QuÃ© hacer ahora:**

1. Reporta el incidente a la autoridad local inmediatamente
2. Presenta evidencia de que fue accidental (fotos, testigos)
3. Solicita un peritaje oficial del daÃ±o
4. Negocia el pago de reparaciÃ³n con el municipio
5. Si tienes seguro, notifÃ­cales del incidente

---

ğŸ‘¨â€âš–ï¸ **Profesionistas especializados en daÃ±os a propiedad:**

1. **Lic. MarÃ­a GonzÃ¡lez Torres** â­â­â­â­â­ (9.8/10)
   ğŸ“ 15 aÃ±os exp. | ğŸ“ Tuxtla GutiÃ©rrez
   âœ… Verificado | ğŸ’¼ 120 casos resueltos

2. **Abg. Carlos LÃ³pez HernÃ¡ndez** â­â­â­â­â­ (9.6/10)
   ğŸ“ 12 aÃ±os exp. | ğŸ“ Chiapas Centro
   âœ… Verificado | ğŸ’¼ 200 casos resueltos

_Toca en las tarjetas para contactar directamente._
```

---

## ğŸ’° AnÃ¡lisis de Costos

### OpciÃ³n 1: t3a.small (Recomendado)

```
Instancia EC2:  $15.33/mes
Storage 20GB:    $2.00/mes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:          $17.33/mes

Consultas/dÃ­a:  50-100
Latencia:       5-10s
Modelo:         Llama3.2-1B
```

### OpciÃ³n 2: t3a.medium (Mejor rendimiento)

```
Instancia EC2:  $30.66/mes
Storage 20GB:    $2.00/mes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:          $32.66/mes

Consultas/dÃ­a:  200-500
Latencia:       2-5s
Modelo:         Llama3.2-3B
```

### ComparaciÃ³n con APIs Externas

| Servicio | Costo 1000 req | Pros | Contras |
|----------|----------------|------|---------|
| **Ollama Local (t3a.small)** | $0.58* | Privacidad, sin lÃ­mites | Infraestructura |
| **Groq API** | $0 (gratis hasta 14k/dÃ­a) | RÃ¡pido, sin infra | LÃ­mites diarios |
| **OpenAI GPT-3.5** | $2.00 | Alta calidad | Costoso |
| **OpenAI GPT-4** | $30.00 | MÃ¡xima calidad | Muy costoso |

\* Basado en $17.33/mes Ã· 30 dÃ­as Ã· 100 consultas/dÃ­a Ã— 1000

---

## ğŸ›ï¸ Toggles y ConfiguraciÃ³n

### Desactivar Ollama temporalmente

```javascript
// En el frontend o backend, al llamar NLP:
{
  "textoConsulta": "tu texto aquÃ­",
  "useOllama": false  // Desactiva Ollama, usa solo diccionario
}
```

### Cambiar modelo segÃºn carga

```bash
# Modelo rÃ¡pido (menos RAM):
docker exec lexia-ollama ollama pull phi3:mini
# Actualizar .env: OLLAMA_MODEL=phi3:mini

# Modelo balanceado (recomendado):
docker exec lexia-ollama ollama pull llama3.2:1b

# Modelo preciso (mÃ¡s RAM):
docker exec lexia-ollama ollama pull llama3.2:3b
```

---

## ğŸ“ˆ MÃ©tricas de Mejora

### Antes (Solo Diccionario)

| MÃ©trica | Valor |
|---------|-------|
| Cobertura lenguaje formal | 95% |
| Cobertura coloquial comÃºn | 40% |
| Cobertura jerga regional | 20% |
| Cobertura casos edge | 5% |
| **Cobertura total** | **~60%** |

### DespuÃ©s (Diccionario + Ollama)

| MÃ©trica | Valor | Mejora |
|---------|-------|--------|
| Cobertura lenguaje formal | 95% | - |
| Cobertura coloquial comÃºn | 85% | +45% |
| Cobertura jerga regional | 75% | +55% |
| Cobertura casos edge | 70% | +65% |
| **Cobertura total** | **~85%** | **+25%** |

### Con Ollama activo en casos edge

| MÃ©trica | Valor | Mejora |
|---------|-------|--------|
| Cobertura total | **98%** | **+38%** |

---

## ğŸ§ª Testing

### Test Manual

```bash
# Test 1: Caso comÃºn (usa diccionario)
curl -X POST http://localhost:3004/process \
  -H "Content-Type: application/json" \
  -d '{"textoConsulta": "me agarraron bolo", "usuarioId": "test123"}'

# Verifica: ollama.used = false

# Test 2: Caso edge (usa Ollama)
curl -X POST http://localhost:3004/process \
  -H "Content-Type: application/json" \
  -d '{"textoConsulta": "destruÃ­ un alumbrado pÃºblico", "usuarioId": "test123"}'

# Verifica: ollama.used = true
```

### Test de Carga

```bash
# Instalar hey
go install github.com/rakyll/hey@latest

# 50 requests, 5 concurrentes
hey -n 50 -c 5 -m POST \
  -H "Content-Type: application/json" \
  -d '{"textoConsulta":"hey destruÃ­ un poste","usuarioId":"load-test"}' \
  http://localhost:3004/process
```

---

## ğŸ”’ Seguridad y Privacidad

### âœ… Ventajas de Ollama Local

1. **Privacidad total:** Datos nunca salen del servidor
2. **Sin lÃ­mites de rate:** No hay restricciones de API
3. **Predictibilidad:** Costo fijo mensual
4. **PersonalizaciÃ³n:** Puedes fine-tunear el modelo

### ğŸ” Recomendaciones

1. âœ… Firewall: Solo abrir puertos necesarios (22, 80, 443)
2. âœ… SSL/TLS: Usar HTTPS en producciÃ³n
3. âœ… Rate limiting: Configurar en Nginx
4. âœ… Logs: Monitorear consultas sospechosas

---

## ğŸ“š PrÃ³ximos Pasos (Opcional)

### 1. Fine-tuning del Modelo

Entrenar Llama3 con dataset especÃ­fico de Chiapas:

```python
# Dataset de entrenamiento
{
  "input": "me agarraron bolo",
  "output": "detenciÃ³n por conducir bajo efectos del alcohol"
},
{
  "input": "destruÃ­ un alumbrado",
  "output": "daÃ±o a propiedad pÃºblica - alumbrado pÃºblico"
}
# ... 1000+ ejemplos
```

### 2. CachÃ© de Respuestas

Cachear normalizaciones comunes para reducir latencia:

```javascript
// Redis cache
const cache = await redis.get(`normalize:${hash(texto)}`);
if (cache) return JSON.parse(cache);
```

### 3. A/B Testing

Comparar respuestas Ollama vs. Diccionario:

```javascript
if (Math.random() < 0.5) {
  // Usar Ollama
} else {
  // Usar solo diccionario
}
// Registrar mÃ©tricas de satisfacciÃ³n
```

---

## ğŸ“ Recursos y DocumentaciÃ³n

- [README Completo](microservices/IA/ollama-preprocessor/README.md)
- [Script de Setup AWS](microservices/IA/ollama-preprocessor/setup-aws.sh)
- [Docker Compose](microservices/IA/docker-compose.ollama.yml)
- [Ollama Docs](https://github.com/ollama/ollama)
- [Llama3.2 Model](https://ollama.com/library/llama3.2)

---

## âœ… Checklist de ImplementaciÃ³n

- [x] âœ… Expandir diccionario NLP con modismos (completado)
- [x] âœ… Crear servicio Ollama Preprocessor (completado)
- [x] âœ… Integrar con NLP service (completado)
- [x] âœ… Docker Compose configurado (completado)
- [x] âœ… DocumentaciÃ³n completa (completado)
- [x] âœ… Script de setup automÃ¡tico (completado)
- [ ] â³ Desplegar en AWS EC2 (pendiente - usuario)
- [ ] â³ Testing en producciÃ³n (pendiente)
- [ ] â³ Monitoreo de mÃ©tricas (pendiente)

---

## ğŸ¤ Soporte

Para preguntas o issues:
1. Revisar [README.md](microservices/IA/ollama-preprocessor/README.md)
2. Verificar logs: `docker-compose -f docker-compose.ollama.yml logs -f`
3. Contactar al equipo de desarrollo

---

**Implementado por:** Claude Code + Tu equipo
**Fecha:** Diciembre 2025
**VersiÃ³n:** 1.0.0
